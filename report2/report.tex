\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{white}, % Set background color
    basicstyle=\ttfamily\footnotesize, % Use a typewriter font
    commentstyle=\color{gray},     % Comment color
    keywordstyle=\color{blue},     % Keyword color
    numberstyle=\tiny\color{gray}, % Line number color
    stringstyle=\color{red},       % String color
    breaklines=true,               % Automatically break long lines
    frame=single,                  % Draw a frame around the code
    numbers=left,                  % Line numbers on the left
    numbersep=5pt,                 % Distance of line numbers from code
    showspaces=false,              % Don't show spaces
    showstringspaces=false,        % Don't show spaces in strings
    showtabs=false,                % Don't show tabs
    tabsize=4                      % Set default tab size
}

% Apply the custom style
\lstset{style=mystyle}

\usepackage{geometry}
\geometry{a4paper, margin=1in}

\usepackage[backend=biber, style=numeric, citestyle=numeric]{biblatex} % Load biblatex with the numeric style
\addbibresource{references.bib} % Specify the database of bibliographic references
\usepackage{hyperref} % For clickable links

\title{TTS}
\author{Davide Giuseppe Griffon}
\date{}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\begin{document}

\maketitle

\begin{abstract}
    This document serves as the report for the second task in the "Natural Language Processing" course completed by student Davide Giuseppe Griffon at Vilnius University as part of the Master's program in Data Science.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}

In the rapidly evolving field of Natural Language Processing (NLP), Text-to-Speech (TTS) synthesis serves as a fundamental technology that bridges the gap between written and spoken language. The primary objective of TTS systems is to generate clear and natural-sounding speech from text, enhancing accessibility for a broader audience, including individuals with visual impairments or speech disorders. Achieving effective TTS synthesis requires an interdisciplinary approach, combining insights from linguistics, digital signal processing, and machine learning to emulate the complexities of human speech production.

Since the first attempts in this area, many modern models aiming to synthesize human speech have relied on vocoders. The \textit{vocoder}, short for ``voice encoder,'' is a critical component that transforms intermediate linguistic or acoustic representations into audible speech waveforms. This technology ensures that synthesized speech is not only intelligible but also carries the natural prosodic features that make human speech expressive and engaging. Understanding the role and evolution of vocoders is essential for appreciating their impact on modern speech synthesis technologies.

\subsection{Traditional Parametric Vocoders}

Traditional parametric vocoders, such as STRAIGHT and WORLD, process speech signals using specific algorithms based on acoustic analysis. They operate by decomposing the speech into key acoustic features. In particular they extract:

\begin{itemize}
    \item \textbf{Spectral Envelope}: This is a smooth curve representing the resonant frequencies (formants) of the vocal tract over time. The spectral envelope characterizes the timbre or color of the speech, reflecting how the shape and movements of the vocal tract affect the sound produced.
    
    \item \textbf{F0 (Fundamental Frequency)}: The fundamental frequency corresponds to the pitch or perceived frequency of the voice. It determines the intonation and melody of speech, playing a vital role in conveying meaning, emotion, and emphasis through variations in pitch.
    
    \item \textbf{Aperiodicity (or Noise Components)}: These components capture the noise and non-periodic parts of the speech signal. They are essential for accurately reproducing unvoiced sounds like fricatives (/s/, /f/), which are characterized by turbulent airflow rather than vocal cord vibrations.
\end{itemize}

After extracting these features, the vocoders synthesize a new waveform by reconstructing the signal based on the analyzed parameters. Although both STRAIGHT and WORLD aim to produce natural-sounding speech from this decomposition, they utilize different algorithms and methods for the analysis and synthesis stages.

These parametric vocoders are valued for their computational efficiency and precise control over speech parameters, making them suitable for real-time applications and environments with limited computational resources. However, a common drawback is that the speech they produce often sounds robotic and lacks the natural expressiveness and subtle nuances inherent in human speech.

\subsection{Neural-Based Vocoders}

The advent of neural vocoders, such as WaveNet and HiFi-GAN, has revolutionized the field by leveraging deep learning techniques to generate highly natural and human-like speech. These models learn complex mappings between acoustic features and waveforms from extensive datasets, capturing subtle nuances in prosody and emotion. While neural vocoders significantly enhance the naturalness of synthesized speech, they demand substantial computational resources and large training datasets.



% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------



\section{Methodology}

In this section, we describe the step-by-step process I followed to implement a vocoder-like method for speech synthesis. The goal is to transform an input speech signal into a Mel spectrogram using the Short-Time Fourier Transform (STFT) and Mel scaling, and then reconstruct the waveform from this spectrogram. The final steps involve saving the reconstructed waveform as a \texttt{.wav} file and analyzing its quality using both objective and subjective methods.

Implementation note: Unlike the first assignment, where I chose not to rely on the \texttt{librosa} library and used only \texttt{numpy}, in this assignment I decided to use only \texttt{librosa} because it already contains many useful functions.

\subsection{Transforming the Speech Signal into a Mel Spectrogram}

The first step involves converting the input audio waveform, which is a time-domain signal, into a frequency-domain representation called a Mel spectrogram. This transformation is essential because it allows us to analyze the frequency content of the audio over time, which is crucial for speech processing.

We start by applying the Short-Time Fourier Transform (STFT) to the audio signal. The STFT divides the signal into short, overlapping time frames, computes the Fourier Transform for each frame, and applies a windowing function to smooth each frame. For this step, I used the \texttt{librosa} function \texttt{librosa.stft}, which performs this task in a single step. While I could have broken this process down into several steps—framing, windowing, and Fourier transformation—the \texttt{librosa} function is so convenient that I chose this simplified approach.

The magnitude of the complex spectrogram obtained from the STFT is then converted to power, and the resulting power signal is transformed into a Mel spectrogram using the function \texttt{librosa.feature.melspectrogram}, which computes a Mel-scaled spectrogram.

The Mel spectrogram is computed by mapping the frequencies of the magnitude spectrogram onto the Mel scale using a set of triangular filters, known as the Mel filter bank. This results in a two-dimensional representation where one axis represents time, and the other represents frequency on the Mel scale.

Humans are better at detecting differences in lower frequencies than in higher frequencies. For example, we can easily tell the difference between 500 Hz and 1000 Hz, but would hardly detect a difference between 10,000 Hz and 10,500 Hz, even though the difference between the two pairs is the same. The Mel scale is designed to align more closely with how humans perceive pitch, giving greater resolution to lower frequencies and compressing higher frequencies.


\subsection{Reconstructing the Waveform from the Mel Spectrogram}

Reconstructing the original audio waveform from the Mel spectrogram is a complex task. For instance, the Mel spectrogram lacks phase information, which is essential for accurate signal reconstruction. Additionally, the Mel scaling compresses the frequency representation, adding further complexity to this process. To manage these challenges, I relied on existing \textit{librosa} functions, specifically:

\begin{itemize}
    \item \textit{librosa.feature.inverse.mel\_to\_stft}, which approximates the STFT magnitude from a Mel power spectrogram.
    \item \textit{librosa.griffinlim}, which performs an approximate magnitude spectrogram inversion using the "fast" Griffin-Lim algorithm.
\end{itemize}

After these two steps, the reconstructed audio waveform is obtained and can be easily saved as a new \texttt{.wav} file. For this step, I used the \texttt{write} function from the \textit{python-soundfile} audio library.

\newpage



% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------



\section{Analysis and Results}
How to write this section: [Describe the methods used to evaluate the quality of the synthesized speech. Present the results of the vocoder evaluations.]

Draft: [
Once the waveform is reconstructed, we save it as a \texttt{.wav} file using the original sample rate. This allows us to perform further analyses to assess the quality of the reconstructed audio.

Some of the analyses include:

\begin{itemize}
    \item \textbf{Visual Analysis}: Plotting the waveform and spectrogram to visually compare them with the original audio.
    \item \textbf{Objective Evaluation}: Calculating metrics such as the Perceptual Evaluation of Speech Quality (PESQ) score to quantitatively assess the audio quality.
    \item \textbf{Subjective Listening Tests}: Listening to the reconstructed audio to evaluate its naturalness and intelligibility from a human perspective.
\end{itemize}

By saving the reconstructed waveform and conducting these analyses, we can evaluate the effectiveness of our vocoder-like method in preserving the essential characteristics of the original speech signal.
]

Results: [

]


\section{Conclusion}
How to write this section: discussion and conclusion regarding the finding

Questions for reflection that will help writing this report:
- How similar does the reconstructed waveform sound to the original?
- Discuss any possible artifacts or differences you noticed. Why might the reconstruction not be perfect?
- Research modern neural vocoders such as HiFi-GAN or WaveNet. How are these vocoders different from the method you implemented? Why do they produce better-quality results?


\newpage


% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------
% -------------------------------------------------------------------------------------------------




\end{document}